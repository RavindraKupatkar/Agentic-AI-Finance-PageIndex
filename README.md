# FinSight â€” Agentic Finance Document RAG

> **An enterprise-grade platform for deep financial document analysis, powered by PageIndex + LangGraph.**

Upload lengthy financial PDFs, interim reports, and annual reviews â€” FinSight builds a hierarchical semantic tree of every document and uses multi-agent LLM reasoning to extract, synthesize, and compare complex financial data with full source citations.

---

## ğŸ“Š Architecture

![PageIndex Finance RAG Architecture](docs/PageIndex_Finance_RAG_Architecture.png)

The system replaces traditional vector-based RAG with **PageIndex** â€” a tree-structured index navigated by LLM reasoning agents orchestrated through **LangGraph**.

### Agentic Query Pipeline

```
User Question
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Guardrails â”‚â”€â”€â”€â–¶â”‚    Router     â”‚â”€â”€â”€â–¶â”‚   Doc Selector   â”‚
â”‚ (input val) â”‚    â”‚ (complexity)  â”‚    â”‚ (which PDFs?)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                         â–¼                    â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   Planner   â”‚    â”‚  Tree Searcher   â”‚
                  â”‚ (multi-hop) â”‚â”€â”€â”€â–¶â”‚ (LLM tree walk)  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â–¼
                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚ Page Extractor   â”‚
                                     â”‚ (PyMuPDF pages)  â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                         â–¼                    â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   Critic    â”‚    â”‚    Generator     â”‚
                  â”‚ (evaluate)  â”‚â”€â”€â”€â–¶â”‚ (final answer)   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â–¼
                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚  Guardrails      â”‚
                                     â”‚ (output + discl.) â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Nodes explained:**

| Node | Role |
|------|------|
| **Guardrails** | Validates input (injection detection, PII filtering) and output (financial disclaimers). |
| **Router** | Classifies query complexity: `simple`, `standard`, `complex`, `multi_hop`. |
| **Doc Selector** | Reads tree root summaries to pick which documents are relevant. |
| **Planner** | For complex/multi-hop queries, generates a step-by-step execution plan. |
| **Tree Searcher** | LLM navigates the PageIndex tree top-down, selecting relevant branches at each level. |
| **Page Extractor** | Extracts exact page text from PDFs via PyMuPDF (replaces chunk retrieval). |
| **Critic** | Scores relevance/groundedness of retrieved content; triggers re-search if low. |
| **Generator** | Synthesizes final answer with inline source citations. |

---

## ğŸ§  PageIndex vs Traditional RAG vs Vector DB

### The Problem with Traditional RAG

Traditional RAG (Retrieval-Augmented Generation) systems chunk documents into fixed-size pieces, embed them into vectors, and retrieve the top-k nearest neighbors. This works for simple lookups but **fails for financial documents** because:

- Fixed-size chunks split tables, footnotes, and multi-page sections mid-sentence.
- Vector similarity misses structural relationships (e.g., "this footnote refers to the balance sheet on page 23").
- No reasoning about *where* to look â€” just brute-force similarity.

### Comparison Table

| Dimension | Traditional RAG (Vector DB) | PageIndex (This System) |
|---|---|---|
| **Index structure** | Flat list of chunks + embeddings | Hierarchical semantic tree (like a smart TOC) |
| **Retrieval method** | Vector similarity (cosine/dot-product) + optional BM25 + reranker | LLM reasoning: top-down tree traversal |
| **Chunking** | Fixed-size (512â€“1024 tokens) or recursive text splitting | No chunking â€” full pages extracted on demand |
| **Cross-page context** | Lost at chunk boundaries | Preserved â€” tree nodes span page ranges |
| **Tables & structured data** | Often split across chunks | Detected per-page, kept intact as Markdown |
| **Multi-document queries** | Retrieve chunks from all docs, hope reranker sorts them | Doc Selector picks relevant documents first, then searches each tree |
| **Explainability** | "These 5 chunks scored highest" | Full reasoning trace: "I checked Financial Statements â†’ Balance Sheet â†’ found debt figures on pages 23â€“30" |
| **Infrastructure** | Requires vector DB (Qdrant/Pinecone/Weaviate) + embedding model | JSON files on disk (or Convex), no vector DB needed |
| **Ingestion cost** | Embed every chunk ($$$ for large corpora) | One LLM call per document to build tree |
| **Query cost** | Embedding + ANN search + reranker | 2â€“4 LLM calls (tree levels) per query |
| **Best for** | Simple fact lookup in homogeneous text | Complex financial analysis, cross-document comparison, regulatory review |

### âœ… Pros of PageIndex

- **No vector database** â€” simpler infra, no embedding model, no similarity tuning
- **Preserves document structure** â€” sections, sub-sections, tables, footnotes stay intact
- **Explainable retrieval** â€” full reasoning trace shows *why* pages were selected
- **Handles long documents** â€” 500+ page annual reports without quality loss
- **Multi-hop reasoning** â€” Planner breaks complex queries into steps, each navigating the tree
- **Exact page extraction** â€” answers cite precise page numbers, not abstract chunk IDs

### âš ï¸ Cons of PageIndex

- Higher per-query latency â€” each tree level requires an LLM call (~2â€“4 calls per query)
- Depends on LLM quality â€” tree generation and search quality are only as good as the model
- Ingestion takes longer â€” building the tree index takes 30â€“120s per document (vs instant embedding)
- Not suited for keyword search â€” pure semantic reasoning, no BM25 fallback
- Tree quality varies â€” documents without clear structure (e.g., scanned images) produce weaker trees

---

## ğŸš€ Features

- **Premium UI/UX** â€” Built with Next.js 16, Framer Motion, GSAP, and Tailwind CSS. Dark mode with orbital animations, liquid glass effects, and 3D magnetic hover cards.
- **PageIndex Architecture** â€” LLM-generated hierarchical tree index replaces vector chunking entirely.
- **Agentic LangGraph Orchestrator** â€” Router â†’ Doc Selector â†’ Planner â†’ Tree Searcher â†’ Critic â†’ Generator pipeline with conditional edges and retry loops.
- **Output Guardrails** â€” Input validation (injection detection, PII filtering), output validation (financial disclaimers).
- **Real-time Auth** â€” Clerk authentication with JWT-protected API endpoints.
- **Convex Data Layer** â€” Real-time database for conversations, messages, documents, and telemetry.
- **Local Observability** â€” SQLite databases for telemetry, conversations, and metadata during development.

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| **Frontend** | Next.js 16 (App Router) + React 19 |
| **Frontend Styling** | Tailwind CSS 4 + Framer Motion + GSAP + Three.js |
| **Authentication** | Clerk (`@clerk/nextjs ^6.39`) |
| **Real-time Data** | Convex (`convex ^1.32`) |
| **Backend API** | FastAPI (Python 3.11+) |
| **AI Orchestration** | LangGraph + LangChain Core |
| **LLM Provider** | Groq (`llama-3.3-70b-versatile` + `llama-3.1-8b-instant`) |
| **Document Processing** | PyMuPDF (text extraction + table detection) |
| **Local Analytics** | SQLite (telemetry, conversations, metadata) |
| **Observability** | OpenTelemetry + Prometheus + Grafana + Tempo |
| **Deployment** | Render (backend) + Vercel (frontend) |

---

## ğŸ Quick Start: Local Development

### Prerequisites

- **Python 3.11+** with `pip`
- **Node.js 18+** with `npm`
- **Git**
- A **Groq API key** â€” free at [console.groq.com](https://console.groq.com)
- A **Clerk account** â€” free at [clerk.com](https://clerk.com) (for authentication)
- A **Convex account** â€” free at [convex.dev](https://convex.dev) (for real-time data)

### 1. Clone & Configure Environment

```bash
git clone https://github.com/RavindraKupatkar/Agentic-AI-Finance-PageIndex-.git
cd "Finance RAG"

# Create Python virtual environment
python -m venv venv

# Activate (Windows PowerShell)
venv\Scripts\activate
# Activate (macOS/Linux)
source venv/bin/activate

# Install Python dependencies
pip install -r requirements.txt
```

### 2. Configure Environment Variables

```bash
cp .env.example .env
```

Edit `.env` and set your keys:

```dotenv
# Required
GROQ_API_KEY=gsk_your_key_here

# Required for Clerk auth
CLERK_SECRET_KEY=sk_test_your_key_here

# Optional â€” Convex deployment URL
CONVEX_URL=https://your-project.convex.cloud
```

### 3. Start the Backend (FastAPI)

```bash
python main.py server
```

The backend API will be available at **`http://localhost:8000/api/v1`**.

Verify it's running:
```bash
curl http://localhost:8000/api/v1/health
```

### 4. Start the Frontend (Next.js)

```bash
# Open a NEW terminal
cd finsight-frontend

# Install dependencies
npm install

# Create frontend environment file
cp .env.example .env.local
# Edit .env.local with your Clerk + Convex keys:
#   NEXT_PUBLIC_CONVEX_URL=https://your-project.convex.cloud
#   NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=pk_test_your_key

# Deploy Convex functions (first time only)
npx convex dev

# Start dev server (in a separate terminal)
npm run dev
```

The frontend will be available at **`http://localhost:3000`**.

### 5. Upload a Document & Query

1. Open `http://localhost:3000` in your browser
2. Sign in with Clerk
3. Navigate to **Documents** â†’ click **Upload PDF**
4. Go to **Chat** â†’ ask a question about the uploaded document

Or via API:
```bash
# Ingest
curl -X POST http://localhost:8000/api/v1/pageindex/ingest -F "file=@report.pdf"

# Query
curl -X POST http://localhost:8000/api/v1/pageindex/query \
  -H "Content-Type: application/json" \
  -d '{"question": "What was the total revenue for 2024?"}'
```

---

## ğŸ” Clerk Authentication

[Clerk](https://clerk.com) handles user authentication for the frontend and JWT verification for the backend.

### Frontend Setup

1. Create a Clerk application at [dashboard.clerk.com](https://dashboard.clerk.com)
2. Copy your **Publishable Key** and **Secret Key**
3. Add to `finsight-frontend/.env.local`:
   ```dotenv
   NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=pk_test_...
   CLERK_SECRET_KEY=sk_test_...
   ```
4. The middleware at `src/middleware.ts` protects `/chat` and `/documents` routes
5. `ConvexClientProvider` wraps the app with `ClerkProvider` + `ConvexProviderWithClerk`

### Backend Setup

1. Add `CLERK_SECRET_KEY=sk_test_...` to your root `.env` file
2. The `verify_clerk_token()` dependency in `src/api/auth.py` validates Clerk JWTs using JWKS
3. If `CLERK_SECRET_KEY` is not set, auth falls back to **mock mode** for local development

---

## ğŸ—„ï¸ Convex Real-time Data Layer

[Convex](https://convex.dev) provides the real-time database powering conversations, documents, and telemetry.

### Schema (6 tables)

| Table | Purpose |
|-------|---------|
| `users` | Synced with Clerk authentication (indexed by `clerkId`) |
| `documents` | Uploaded PDFs and their indexing status |
| `trees` | PageIndex JSON tree structures per document |
| `conversations` | Chat threads per user |
| `messages` | Individual messages within conversations |
| `telemetry` | System events and analytics |

### Convex Setup

1. Create a Convex project at [dashboard.convex.dev](https://dashboard.convex.dev)
2. Inside `finsight-frontend/`, run:
   ```bash
   npx convex dev
   ```
   This deploys the schema and functions from the `convex/` directory.
3. Copy the deployment URL to `finsight-frontend/.env.local`:
   ```dotenv
   NEXT_PUBLIC_CONVEX_URL=https://your-project-123.convex.cloud
   ```
4. **Clerk integration**: In the Convex dashboard, go to Settings â†’ Authentication â†’ Add Clerk
5. Set your `CLERK_ISSUER_URL` in the Convex environment variables

### Local Development Note

For local development, the backend uses **SQLite** databases for:
- `data/telemetry.db` â€” query events, latency tracking, error logs
- `data/conversations.db` â€” conversation history (backend-side)
- `data/pageindex_metadata.db` â€” document indexing metadata

This dual approach gives you:
- **Local dev**: SQLite for fast, zero-config logging and debugging
- **Production**: Convex for real-time sync, multi-user support, and cloud deployment

---

## ğŸ§ª Testing

### Unit Tests (pytest)

```bash
# Activate virtual environment
venv\Scripts\activate

# Run all unit tests
pytest tests/ -v

# Run with coverage
pytest tests/ -v --cov=src --cov-report=term-missing

# Run a specific test file
pytest tests/test_state.py -v
```

### Integration Tests

```bash
# Ensure the backend server is running first
python main.py server

# In a new terminal, run the comprehensive suite
python test_comprehensive.py
```

Test results are exported to `comprehensive_test_report.md`.

---

## ğŸ“ Project Structure

```
Finance RAG/
â”œâ”€â”€ main.py                          # Entry point (server / ingest CLI)
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ docker-compose.yaml              # Full stack with observability
â”œâ”€â”€ Dockerfile                       # Production container
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                         # FastAPI application
â”‚   â”‚   â”œâ”€â”€ main.py                  # App factory, CORS, middleware
â”‚   â”‚   â”œâ”€â”€ auth.py                  # Clerk JWT verification
â”‚   â”‚   â”œâ”€â”€ routes/pageindex.py      # /ingest, /query, /documents
â”‚   â”‚   â”œâ”€â”€ middleware/              # Rate limiting, error handling
â”‚   â”‚   â””â”€â”€ schemas/                 # Request/response models
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/                      # LangGraph agent pipeline
â”‚   â”‚   â”œâ”€â”€ nodes/                   # Router, Planner, TreeSearch, Critic, Generator, Guardrails
â”‚   â”‚   â””â”€â”€ schemas/state.py         # TypedDict states + Pydantic models
â”‚   â”‚
â”‚   â”œâ”€â”€ pageindex/                   # Core PageIndex implementation
â”‚   â”‚   â”œâ”€â”€ page_extractor.py        # PDF page text extraction (PyMuPDF)
â”‚   â”‚   â”œâ”€â”€ tree_generator.py        # LLM-based tree index creation
â”‚   â”‚   â””â”€â”€ tree_searcher.py         # LLM reasoning tree search
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/groq_client.py          # Groq API client with retry
â”‚   â”œâ”€â”€ core/config.py              # Pydantic Settings configuration
â”‚   â”œâ”€â”€ services/convex_service.py   # Convex Python client
â”‚   â””â”€â”€ observability/               # Logging, metrics, telemetry, tracing
â”‚
â”œâ”€â”€ finsight-frontend/               # Next.js 16 frontend
â”‚   â”œâ”€â”€ src/app/                     # Pages: landing, chat, documents
â”‚   â”œâ”€â”€ src/components/              # Shared UI components
â”‚   â”œâ”€â”€ src/middleware.ts            # Clerk route protection
â”‚   â””â”€â”€ convex/                      # Schema + CRUD functions
â”‚
â”œâ”€â”€ tests/                           # pytest unit tests
â”œâ”€â”€ data/                            # PDFs + tree JSONs
â””â”€â”€ docs/                            # Architecture diagrams
```

---

## ğŸ“ License

This project is licensed under the MIT License.
